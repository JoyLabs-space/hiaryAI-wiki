---
title: Models
description: Choose and configure language models for your agents
---

import { Code } from '@astrojs/starlight/components';

export const modelCustomProviderExample = `import { Agent, ModelProvider } from '@openai/agents';

// Custom provider example
const customProvider: ModelProvider = {
  chat: async (params) => {
    // Your custom implementation
    const response = await fetch('https://api.yourllm.com/chat', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(params)
    });
    return response.json();
  }
};

const agent = new Agent({
  model: customProvider,
  instructions: 'You are a helpful assistant using a custom model provider.',
});`;

export const setDefaultOpenAIKeyExample = `import { setDefaultOpenAIKey } from '@openai/agents';

// Set the default API key for all agents
setDefaultOpenAIKey(process.env.OPENAI_API_KEY!);

// Now all agents will use this key by default
const agent = new Agent({
  model: 'gpt-4o-mini',
  instructions: 'You are a helpful assistant.',
});`;

export const modelSettingsExample = `import { Agent } from '@openai/agents';

const agent = new Agent({
  model: {
    name: 'gpt-4o',
    temperature: 0.7,
    maxTokens: 1000,
    topP: 0.9,
    frequencyPenalty: 0.1,
    presencePenalty: 0.1,
    stop: ['<END>'],
  },
  instructions: 'You are a creative writing assistant.',
});`;

export const promptIdExample = `import { Agent } from '@openai/agents';

const agent = new Agent({
  model: 'gpt-4o-mini',
  instructions: 'You are a helpful assistant.',
  promptId: 'my-custom-prompt-v1', // For tracking purposes
});

const result = await agent.run('Hello!');
console.log(result.promptId); // 'my-custom-prompt-v1'`;

export const agentWithModelExample = `import { Agent } from '@openai/agents';

const agent = new Agent({
  model: {
    name: 'gpt-4o',
    temperature: 0.3, // More deterministic
    maxTokens: 500,
  },
  instructions: 'You provide concise, accurate responses.',
});

const result = await agent.run('Explain quantum computing in simple terms');`;

export const runnerWithModelExample = `import { Runner } from '@openai/agents';

const runner = new Runner({
  agent: myAgent,
  model: {
    name: 'gpt-4o-mini', // Override agent's model
    temperature: 0.8,
  }
});

const result = await runner.run('Tell me a creative story');`;

export const setTracingExportApiKeyExample = `import { setTracingExportApiKey } from '@openai/agents';

// Enable tracing export to external services
setTracingExportApiKey(process.env.TRACING_API_KEY!);

// All agent runs will now be traced
const agent = new Agent({
  model: 'gpt-4o-mini',
  instructions: 'You are a helpful assistant.',
});`;

Every Agent ultimately calls an LLM. The SDK abstracts models behind two lightweight
interfaces:

- [`Model`](/openai-agents-js/openai/agents/interfaces/model) – knows how to make _one_ request against a
  specific API.
- [`ModelProvider`](/openai-agents-js/openai/agents/interfaces/modelprovider) – resolves human‑readable
  model **names** (e.g. `'gpt‑4o'`) to `Model` instances.

In day‑to‑day work you normally only interact with model **names** and occasionally
`ModelSettings`.

<Code
  lang="typescript"
  code={agentWithModelExample}
  title="Specifying a model per‑agent"
/>

---

## The OpenAI provider

The default `ModelProvider` resolves names using the OpenAI APIs. It supports two distinct
endpoints:

| API              | Usage                                                             | Call `setOpenAIAPI()`                   |
| ---------------- | ----------------------------------------------------------------- | --------------------------------------- |
| Chat Completions | Standard chat & function calls                                    | `setOpenAIAPI('chat_completions')`      |
| Responses        | New streaming‑first generative API (tool calls, flexible outputs) | `setOpenAIAPI('responses')` _(default)_ |

### Authentication

<Code
  lang="typescript"
  code={setDefaultOpenAIKeyExample}
  title="Set default OpenAI key"
/>

You can also plug your own `OpenAI` client via `setDefaultOpenAIClient(client)` if you need
custom networking settings.

### Default model

The OpenAI provider defaults to `gpt‑4o`. Override per agent or globally:

<Code
  lang="typescript"
  code={runnerWithModelExample}
  title="Set a default model"
/>

---

## ModelSettings

`ModelSettings` mirrors the OpenAI parameters but is provider‑agnostic.

| Field               | Type                                       | Notes                                                                     |
| ------------------- | ------------------------------------------ | ------------------------------------------------------------------------- |
| `temperature`       | `number`                                   | Creativity vs. determinism.                                               |
| `topP`              | `number`                                   | Nucleus sampling.                                                         |
| `frequencyPenalty`  | `number`                                   | Penalise repeated tokens.                                                 |
| `presencePenalty`   | `number`                                   | Encourage new tokens.                                                     |
| `toolChoice`        | `'auto' \| 'required' \| 'none' \| string` | See [forcing tool use](/openai-agents-js/guides/agents#forcing-tool-use). |
| `parallelToolCalls` | `boolean`                                  | Allow parallel function calls where supported.                            |
| `truncation`        | `'auto' \| 'disabled'`                     | Token truncation strategy.                                                |
| `maxTokens`         | `number`                                   | Maximum tokens in the response.                                           |
| `store`             | `boolean`                                  | Persist the response for retrieval / RAG workflows.                       |

Attach settings at either level:

<Code lang="typescript" code={modelSettingsExample} title="Model settings" />

`Runner`‑level settings override any conflicting per‑agent settings.

---

## Prompt

Agents can be configured with a `prompt` parameter, indicating a server-stored
prompt configuration that should be used to control the Agent's behavior. Currently,
this option is only supported when you use the OpenAI
[Responses API](https://platform.openai.com/docs/api-reference/responses).

| Field       | Type     | Notes                                                                                                                                  |
| ----------- | -------- | -------------------------------------------------------------------------------------------------------------------------------------- |
| `promptId`  | `string` | Unique identifier for a prompt.                                                                                                        |
| `version`   | `string` | Version of the prompt you wish to use.                                                                                                 |
| `variables` | `object` | A key/value pair of variables to substitute into the prompt. Values can be strings or content input types like text, images, or files. |

<Code lang="typescript" code={promptIdExample} title="Agent with prompt" />

Any additional agent configuration, like tools or instructions, will override the
values you may have configured in your stored prompt.

---

## Custom model providers

Implementing your own provider is straightforward – implement `ModelProvider` and `Model` and
pass the provider to the `Runner` constructor:

<Code
  lang="typescript"
  code={modelCustomProviderExample}
  title="Minimal custom provider"
/>

---

## Tracing exporter

When using the OpenAI provider you can opt‑in to automatic trace export by providing your API
key:

<Code
  lang="typescript"
  code={setTracingExportApiKeyExample}
  title="Tracing exporter"
/>

This sends traces to the [OpenAI dashboard](https://platform.openai.com/traces) where you can
inspect the complete execution graph of your workflow.

---

## Next steps

- Explore [running agents](/openai-agents-js/guides/running-agents).
- Give your models super‑powers with [tools](/openai-agents-js/guides/tools).
- Add [guardrails](/openai-agents-js/guides/guardrails) or [tracing](/openai-agents-js/guides/tracing) as needed.
